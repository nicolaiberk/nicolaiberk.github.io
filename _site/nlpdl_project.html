<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />



<meta name="date" content="2022-07-08" />

<title>Classifying Newspaper Bias with Cross-Domain Learning</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


<!-- Add Lato font -->
<link href='https://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>




<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Nicolai Berk</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">About</a>
</li>
<li>
  <a href="pubs.html">Academic Work</a>
</li>
<li>
  <a href="https://www.dropbox.com/sh/xar45ns5igvprca/AACU7Fq2kCl25AN6kfGxDHkAa?dl=0&amp;preview=CV.pdf">CV</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Classifying Newspaper Bias with
Cross-Domain Learning</h1>
<h4 class="author">Nicolai Berk</h4>
<h4 class="author">Tom Arend</h4>
<h4 class="date">2022-07-08</h4>

</div>


<div id="problem" class="section level2">
<h2>Problem</h2>
<p>Often, political scientists want to measure ideological bias in
political texts, such as newspapers. While supervised techniques provide
an efficient tool to address this problem, we often lack available
labels to train these supervised models. Additionally, the labelling of
political slant is often not straightforward and affected by coders’
biases.</p>
<p>We propose to use information from political texts with available
labels - namely the press releases of political parties - to assess the
relative placement of political texts with transformer models <span
class="citation">(Devlin et al. 2019)</span>. This cross-domain approach
should enable researchers to assess the political bias in content that
is costly or hard to label. Moving beyond existing approaches which
infer political bias from the issuing actors <span
class="citation">(Widmer, Ash, and Galletta 2020)</span> or specific
phrases used by politicians <span class="citation">(Gentzkow and Shapiro
2010)</span>, we apply state-of-the-art transformer models to measure
the similarity of newspaper articles to the language of political
parties.</p>
</div>
<div id="task" class="section level2">
<h2>Task</h2>
<p>To understand how information from political actors can be used to
understand ideological bias in other context, we train several
DistilBERT transformer models<a href="#fn1" class="footnote-ref"
id="fnref1"><sup>1</sup></a> on a collection of over 40,000 German party
press releases issued between 2013 and 2019, collected by the SCRIPTS
project<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>.
These are then applied to a random subsample of 4,000 German newspaper
articles from six major newspapers, published between 2013 and 2019,
collected by one of the authors in a previous project<a href="#fn3"
class="footnote-ref" id="fnref3"><sup>3</sup></a>. To validate whether
the estimates from our classifiers conform to general expectations about
the ideological bias of these newspapers, we use data from a survey of
newspaper readers asking them about the partisan bias of their newspaper
<span class="citation">(Roßteutscher et al. 2019)</span>.</p>
</div>
<div id="proposed-method" class="section level2">
<h2>Proposed method</h2>
<p>We assess the use of cross-domain learning to identify newspaper
slant in two main studies<a href="#fn4" class="footnote-ref"
id="fnref4"><sup>4</sup></a>. A pre-trained transformers model is
fine-tuned on our set of press releases to identify the authoring
party.</p>
<p>In study 1, we use the DistilBERT model with different training and
test sets and compare performance: first, we train on the full set of
releases issued between 2013 and 2019 and assess the performance on a
test set randomly chosen from this data<a href="#fn5"
class="footnote-ref" id="fnref5"><sup>5</sup></a>. Second, to exclude
the possibility that the model only infers from references to the
political parties, we ‘blindfold’ the classifier by censoring any
references to the parties. Lastly, we split training and test set
temporally, meaning the classifier needs to predict the labels of press
releases published after the period from which the training data was
sampled. If the model picks up general patterns about the parties’
language, the performance on this set should only decrease slightly,
meaning that the model should correctly predict the authoring party of
the press release most of the time. Comparing performance using these
different training and validation sets, we can assess whether the
classifiers perform as expected.</p>
<p>In study 2, these models are applied to a range of newspaper
articles, indicating which parties’ communication an article most
resembles. These estimates are compared to the expectations about
newspaper slant formulated by the survey respondents. Ideally, the
models show little respnsiveness to changes in the input data and
closely resemble the survey estimates.</p>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<p><strong>Study 1</strong>:</p>
<p>As described, we start by assessing the model’s performance on a set
of press releases issued in the same period as the training data. The
results are shown in <a href="#fig:perf_init" reference-type="ref"
reference="fig:perf_init">the table</a>. As the reader can see, the
model performs at a very high, near-perfect level for all categories.
The impressive performance of this model on the press releases was
rather surprising. Indeed, this performance seemed too good to be true
and raised a number of concerns, most notably regarding issues of
overfitting.</p>
<dl>
<dt><img src="pics/party_class.png" style="width:60.0%"
alt="Distilbert model performance on unaltered in-sample test set" /></dt>
<dd>
Table: Distilbert model performance on unaltered in-sample test
set.<span id="fig:perf_init" label="fig:perf_init"></span>
</dd>
</dl>
<p>To assess whether this is the case, we "blindfolded" our model by
excluding party labels and any words referring to the parties (such as
party-related colours) from the analysis <a href="#fn6"
class="footnote-ref" id="fnref6"><sup>6</sup></a>. Additionally, we look
at out-of-sample predictions to assess whether the model is overfitting.
This means that we train the model (as before) on press releases
published before 01.01.2018, but assess the performance on data after
this date. If the model performance drops off significantly, this would
suggest that the classifier picks up and incorporates party-specific
clues or speech patterns that could artificially inflate its accuracy.
Should the model continue to perform at such a high level, we would have
to start considering that the model is indeed this accurate.</p>
<p>For the in-sample, blindfolded set, performance is virtually
identical. This is very, very surprising, given that the most obvious
cues - references to other parties - are missing in the training data.
Have been removed. This indicateone of three things. First, the
performance might accurately describe the classifiers ability to
distinguish partisan differences in language. We will discuss this
option further in a moment. Second, we might have missed some obvious
cues in the blindfolding process. While our dictionary of party
references is rather extensive, it might still omit important cues that
we did not think of. A third option is that BERT is too powerful,
learning some spurious statistical cues to place party press releases
with parties <span class="citation">(Niven and Kao 2019)</span>.</p>
<p>As stated previously, this performance, while impressive, will likely
not suffice to convince us that no overfitting is taking place. We
therefor use this model to predict the authoring party in an out-of
sample dataset. If the performance is strongly affected, we are
overfitting on the training set. <a href="#fig:perf_out"
reference-type="ref" reference="fig:perf_out">The table below</a> shows
the performance for this test sample. As we can see, the performance is
still very high for many parties, nearly perfectly predicting press
releases issued bythe Greens, Linke, and Union. AfD and FDP show
somewhat decreased but still good performance, although the recall for
FDP press releases is not great at .72. However, the precision is
severely decreased for the SPD at about .52. Inspection of the cross
tabulation for predicted and actual labels (not shown) indicates that
the classifier confuses AfD and FDP press releases for SPD press
releases.</p>
<p>These results do not give a definite answer to whether the classifier
performs as intended. The numbers just seem a little too good. Given
that AfD and SPD occupy opposite ends of the ideological spectrum and
that one is an opposition, the other a governing party, a
well-performing classifier should not confuse the two. It is thus more
likely that the classifier picks up spurious relationships in the data
that we do not understand. However, before concluding, we will assess
whether the newspaper estimates produced by the DistilBert model are in
line with our expectations and stable across different
specifications.</p>
<dl>
<dt><img src="pics/blindfolded_outsample.png" style="width:60.0%"
alt="Distilbert Model performance on out-of-sample test set with party references removed." /></dt>
<dd>
Table: Distilbert Model performance on out-of-sample test set with party
references removed.<span id="fig:perf_out" label="fig:perf_out"></span>
</dd>
</dl>
<p><strong>Study 2</strong>:</p>
<p>We now move on to assess the estimates for newspaper bias produced by
these different models, classifying a set of 4,000 newspaper articles.
We can generate a baseline expectation using for the similarity of each
newspaper to different parties using survey data from the 2017 German
Longitudinal Election Study’s Rolling Cross-Section <span
class="citation">(Roßteutscher et al. 2019)</span>. The study asked
respondents whether they read a daily newspaper, and if so, whether they
felt the newspaper’s reporting favored certain parties over others.
While these estimates are likely biased by the respondents own political
stances (and as such likely underestimate the ideological extremity of
their paper), they do provide a first clue.</p>
<p><a href="#tab:gles" reference-type="ref" reference="tab:gles">The
table</a> shows the aggregated results of this question. We can see that
FAZ and Welt are placed closer to the centre-right Union, but also SPD
and FDP, while the more left-wing TAZ is placed close to the three
centre-left and left parties (Linke, Grüne, SPD). None of the newspapers
is considered to contain coverage particularly favorable for the
AfD.</p>
<table>
<caption>Validation reference: readers’ assessment of newspapers’
bias<span label="tab:gles"></span></caption>
<thead>
<tr class="header">
<th align="left">Paper</th>
<th align="center">Linke</th>
<th align="center">Grüne</th>
<th align="center">SPD</th>
<th align="center">Union</th>
<th align="center">FDP</th>
<th align="center">AfD</th>
<th align="center">N</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FAZ</td>
<td align="center"><span class="math inline">\(0.01\)</span></td>
<td align="center"><span class="math inline">\(0.02\)</span></td>
<td align="center"><span class="math inline">\(0.11\)</span></td>
<td align="center"><span class="math inline">\(0.33\)</span></td>
<td align="center"><span class="math inline">\(0.11\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(390\)</span></td>
</tr>
<tr class="even">
<td align="left">TAZ</td>
<td align="center"><span class="math inline">\(0.23\)</span></td>
<td align="center"><span class="math inline">\(0.24\)</span></td>
<td align="center"><span class="math inline">\(0.27\)</span></td>
<td align="center"><span class="math inline">\(0.16\)</span></td>
<td align="center"><span class="math inline">\(0.04\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(89\)</span></td>
</tr>
<tr class="odd">
<td align="left">Welt</td>
<td align="center"><span class="math inline">\(0.01\)</span></td>
<td align="center"><span class="math inline">\(0.02\)</span></td>
<td align="center"><span class="math inline">\(0.18\)</span></td>
<td align="center"><span class="math inline">\(0.35\)</span></td>
<td align="center"><span class="math inline">\(0.09\)</span></td>
<td align="center"><span class="math inline">\(0.03\)</span></td>
<td align="center"><span class="math inline">\(209\)</span></td>
</tr>
</tbody>
</table>
<p>Based on this expectation, we can assess the validity of the
classifier’s estimates of newspaper bias.</p>
<table>
<caption>Mean similarity estimate to each party by newspaper, based on
training data including party labels.<span
label="tbl:means"></span></caption>
<thead>
<tr class="header">
<th align="left">Paper</th>
<th align="center">Linke</th>
<th align="center">Grüne</th>
<th align="center">SPD</th>
<th align="center">Union</th>
<th align="center">FDP</th>
<th align="center">AfD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FAZ</td>
<td align="center"><span class="math inline">\(0.15\)</span></td>
<td align="center"><span class="math inline">\(0.36\)</span></td>
<td align="center"><span class="math inline">\(0.05\)</span></td>
<td align="center"><span class="math inline">\(0.81\)</span></td>
<td align="center"><span class="math inline">\(0.58\)</span></td>
<td align="center"><span class="math inline">\(0.08\)</span></td>
</tr>
<tr class="even">
<td align="left">TAZ</td>
<td align="center"><span class="math inline">\(0.13\)</span></td>
<td align="center"><span class="math inline">\(0.69\)</span></td>
<td align="center"><span class="math inline">\(0.04\)</span></td>
<td align="center"><span class="math inline">\(0.41\)</span></td>
<td align="center"><span class="math inline">\(0.32\)</span></td>
<td align="center"><span class="math inline">\(0.05\)</span></td>
</tr>
<tr class="odd">
<td align="left">Welt</td>
<td align="center"><span class="math inline">\(0.15\)</span></td>
<td align="center"><span class="math inline">\(0.35\)</span></td>
<td align="center"><span class="math inline">\(0.05\)</span></td>
<td align="center"><span class="math inline">\(0.81\)</span></td>
<td align="center"><span class="math inline">\(0.55\)</span></td>
<td align="center"><span class="math inline">\(0.07\)</span></td>
</tr>
</tbody>
</table>
<p><a href="#tbl:means" reference-type="ref" reference="tbl:means">The
table above</a> shows the average probability of party authorship
assigned by the initial, uncesored in-sample classifier. As expected,
FAZ and Welt are very similar to the centre-right Union (81%) and FDP
(58%/55%), but also rather similar to the Greens (36%/35%). While they
show the highest similarity to the AfD (8%/7%), similarity to the
radical-right party is generally on a very low level among all
newspapers. As expected, the TAZ shows a comparatively different
profile, being very similar to the Greens (average likelihood 70%), and
less similar to the Union parties (41%), the FDP (32%), and the AfD
(5%). Surprisingly, it also shows the lowest similarity to the Linke
(13%). Maybe most surprising is the general low similarity to SPD press
releases (4%/5%). It seems the party has a rather distinctive rhetorical
style.</p>
<p>To see if these estimates are robust across training, we first use
the blindfolded classifier, before temporally specifying the train-test
split. The results of these experiments can be seen in <a
href="#tab:pred_blind">the table below</a>. The minor changes in the
training data severely affected our results. Resemblance of all
newspapers towards Greens and Union have strongly decreased, while
resemblance to Linke, SPD, AfD, and especially FDP has strongly
increased. All newspapers (including the left-wing TAZ) are now
estimated to strongly resemble the FDP instead of the Union. While the
higher estimates for SPD are welcomed and in line with readers’
assessment, the strong resemblance with the FDP is puzzling, as are the
relatively high scores for the AfD and low scores for the Union. The
Greens are now least represented in all newspapers. Even the left-wing
TAZ bears stronger similarities to the far-right AfD than the Grüne. The
right-wing newspapers are now showing the highest similarity to the SPD,
which is non-intuitive and in direct contradicition to our survey data.
All newspapers show very weak resemblance to Union press releases, again
contradicting the survey data<a href="#fn7" class="footnote-ref"
id="fnref7"><sup>7</sup></a>.</p>
<table>
<caption>Placement of newspapers using time-restricted and blindfolded
classifier.<span id="tab:pred_blind"
label="tab:pred_blind"></span></caption>
<thead>
<tr class="header">
<th align="left">Paper</th>
<th align="center">Linke</th>
<th align="center">Grüne</th>
<th align="center">SPD</th>
<th align="center">Union</th>
<th align="center">FDP</th>
<th align="center">AfD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FAZ</td>
<td align="center"><span class="math inline">\(0.17\)</span></td>
<td align="center"><span class="math inline">\(0.08\)</span></td>
<td align="center"><span class="math inline">\(0.35\)</span></td>
<td align="center"><span class="math inline">\(0.10\)</span></td>
<td align="center"><span class="math inline">\(0.95\)</span></td>
<td align="center"><span class="math inline">\(0.20\)</span></td>
</tr>
<tr class="even">
<td align="left">TAZ</td>
<td align="center"><span class="math inline">\(0.62\)</span></td>
<td align="center"><span class="math inline">\(0.08\)</span></td>
<td align="center"><span class="math inline">\(0.23\)</span></td>
<td align="center"><span class="math inline">\(0.07\)</span></td>
<td align="center"><span class="math inline">\(0.60\)</span></td>
<td align="center"><span class="math inline">\(0.17\)</span></td>
</tr>
<tr class="odd">
<td align="left">Welt</td>
<td align="center"><span class="math inline">\(0.16\)</span></td>
<td align="center"><span class="math inline">\(0.11\)</span></td>
<td align="center"><span class="math inline">\(0.31\)</span></td>
<td align="center"><span class="math inline">\(0.10\)</span></td>
<td align="center"><span class="math inline">\(0.93\)</span></td>
<td align="center"><span class="math inline">\(0.20\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="analysis" class="section level2">
<h2>Analysis</h2>
<p>While all classifiers perform well on the party press releases and
correctly place the newspapers relative to each other, the comparison of
classifiers reveals a startling instability in the estimates. Small
changes in the input data resulted in vastly different estimates of
newspaper bias. It seems that cross-domain applications of deep learning
are highly sensitive to the form of the input data and that their
application is not as straightforward as hoped. For the moment, it is
unclear whether this method can provide stable estimates.</p>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>The recommendation at this point is that researchers with
cross-domain classification problems think hard about their input data,
carefully validating their results, or apply simpler techniques where
the estimates are more directly interpretable, such as regression.
Nevertheless, cross-domain applications of deep learning are a promising
avenue for further research. Future work here should assess how
properties of the input data affect estimates in another domain and
think carefully about validation to develop best practices for
researches seeking to apply such methods.</p>
</div>
<div id="acknowledgments" class="section level2">
<h2>Acknowledgments</h2>
<p>We thank the vast community contributing to the development of
machine learning tools, specifically huggingface. Additionally, we want
to thank Lukas Stötzer for provision of the training data and Hauke
Licht for providing us with a first notebook. Most importantly, we want
to thank Slava Jankin, Hannah Bechara, and Huy Dang for providing us
with the skills to pursue this project.</p>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Devlin2019" class="csl-entry">
Devlin, Jacob, Ming Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
<span>“<span class="nocase">BERT: Pre-training of deep bidirectional
transformers for language understanding</span>.”</span> <em>NAACL HLT
2019 - 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies - Proceedings
of the Conference</em> 1 (Mlm): 4171–86. <a
href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>.
</div>
<div id="ref-Gentzkow2010" class="csl-entry">
Gentzkow, Matthew, and Jesse M Shapiro. 2010. <span>“<span>What Drives
Media Slant? Evidence From U.S. Daily Newspapers</span>.”</span>
<em>Econometrica</em> 78 (1): 35–71. <a
href="https://doi.org/10.3982/ecta7195">https://doi.org/10.3982/ecta7195</a>.
</div>
<div id="ref-DBLP" class="csl-entry">
Niven, Timothy, and Hung-Yu Kao. 2019. <span>“Probing Neural Network
Comprehension of Natural Language Arguments.”</span> <em>CoRR</em>
abs/1907.07355. <a
href="http://arxiv.org/abs/1907.07355">http://arxiv.org/abs/1907.07355</a>.
</div>
<div id="ref-GLES2017RCS" class="csl-entry">
Roßteutscher, Sigrid, Harald Schoen, Rüdiger Schmitt-Beck, Bernhard
Weßels, Christof Wolf, and Alexander Staudt. 2019. <span>“<span
class="nocase">Rolling Cross-Section Wahlkampfstudie mit
Nachwahl-Panelwelle (GLES 2017)</span>.”</span> GESIS Datenarchiv,
K<span>ö</span>ln. <a
href="https://doi.org/ZA6803 Datenfile Version 4.0.1, 10.4232/1.13213.">https://doi.org/ZA6803
Datenfile Version 4.0.1, 10.4232/1.13213.</a>
</div>
<div id="ref-Widmer2020" class="csl-entry">
Widmer, Philine, Elliott Ash, and Sergio Galletta. 2020. <span>“<span
class="nocase">Media Slant is Contagious</span>.”</span> <em>SSRN
Electronic Journal</em>, 1–42. <a
href="https://doi.org/10.2139/ssrn.3712218">https://doi.org/10.2139/ssrn.3712218</a>.
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p><a
href="https://huggingface.co/distilbert-base-german-cased"
class="uri">https://huggingface.co/distilbert-base-german-cased</a><a
href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><a href="https://www.scripts-berlin.eu/"
class="uri">https://www.scripts-berlin.eu/</a><a href="#fnref2"
class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p><a href="https://github.com/nicolaiberk/_rrpviol_med"
class="uri">https://github.com/nicolaiberk/_rrpviol_med</a><a
href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Note that the paper distinguished three studies. For the
sake of simplicity, we clump the first two into one<a href="#fnref4"
class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>This test set was of course ot used in training.<a
href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>the full list of excluded terms can be found in Appendix
A of the final report<a href="#fnref6"
class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>Assessing data with no blindfolding but temporal
restrictions affects the model in a similar way as below, but with
higher estimates for AfD and Greens, and lower estimates for FDP.<a
href="#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

&nbsp;


<div class = "col-md-8">
<hr>
  
<p style="text-align: center;">
  <a href = "nicolaiberk.com">Nicolai Berk</a>
  <br>
    <a href="https://github.com/nicolaiberk" class="fa fa-github"></a>
    <a href="https://scholar.google.de/citations?user=1mQThXgAAAAJ%26hl=en%26oi=ao" class="fa fa-graduation-cap"></a>
    <a href="https://twitter.com/nicolaiberk" class="fa fa-twitter"></a>
    <a href="mailto:nicolai.berk@gmail.com" class="fa fa-envelope"></a>
</p>
</div>

&nbsp;



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
